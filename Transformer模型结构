import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import math
import copy
'''Transformer模型的组块包括携带位置信息的嵌入层、Encoder、Decoder和Generator，以下分别予以构建。'''
input_seq=torch.nn.utils.rnn.pad_sequence(input_seq,batch_first=True,padding_value=0.0) #将input_seq中每个句子都填充到最长句子的尺寸，这一步在数据处理阶段实现，确保输入嵌入层的input_seq尺寸为(batch_size,max_len)
class EmbeddingwithPositionalEncoding(nn.Module): #携带位置信息的嵌入层，包括填充处理、嵌入为稠密向量、稠密向量与位置编码合并等组件
    def __init__(self,vocab_size,d_model,max_len,p_dropout):
        super(EmbeddingwithPositionalEncoding,self).__init__()
        self.embedding=nn.Embedding(vocab_size,d_model)
        pe=torch.zeros((max_len,d_model))
        for pos in range(0,max_len):
            for i in range(0,d_model,2):
                pe[pos,i]=torch.sin(pos/(10000**(i/d_model)))
                pe[pos,i+1]=torch.cos(pos/(10000**(i/d_model)))
        pe=pe.unsqueeze(0) #pe尺寸为(1,max_len,d_model)
        self.register_buffer('pe',pe) #将位置编码注册为缓冲信息，在训练时不随动态计算图更新
        self.dropout=nn.Dropout(p=p_dropout)
        self.max_len=max_len
    def forward(self,input_seq):
        input_seq=self.embedding(input_seq)+self.pe[:,:input_seq.size(1)]
        input_seq=self.dropout(input_seq)
        return input_seq #input_seq为携带位置信息的嵌入张量，尺寸为(batch_size,len_seq,d_model)
def attention(query,key,value,mask=None,dropout=None): #定义自注意力矩阵、交叉注意力矩阵中注意力计算部分，由于在两种注意力矩阵的计算过程中应用不同的mask，这里定义mask默认为None
    attn_scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(query.size(-1)) #未经掩码机制及概率分布化处理的注意力分数，如果query=key=value则输出自注意力分数，如果query与key、value不同则是encoder-decoder交叉注意力分数
    if mask is not None:
        attn_scores=attn_scores.mask_fill(mask==0,-1e9)
        p_attn=F.softmax(attn_scores,dim=-1)
        if dropout is not None:
            p_attn=dropout(p_attn)
        return torch.matmul(p_attn,value),p_attn
