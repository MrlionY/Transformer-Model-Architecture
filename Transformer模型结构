import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
import numpy as np
import math
import copy
'''Transformer模型的组块包括携带位置信息的嵌入层、Encoder、Decoder和Generator，以下分别予以构建。'''
input_seq=torch.nn.utils.rnn.pad_sequence(input_seq,batch_first=True,padding_value=0.0) #将input_seq中每个句子都填充到最长句子的尺寸，这一步在数据处理阶段实现，确保输入嵌入层的input_seq尺寸为(batch_size,max_len)
class EmbeddingwithPositionalEncoding(nn.Module): #携带位置信息的嵌入层模块，包括填充处理、嵌入为稠密向量、稠密向量与位置编码合并等组件
    def __init__(self,vocab_size,d_model,max_len,p_dropout):
        super(EmbeddingwithPositionalEncoding,self).__init__()
        self.embedding=nn.Embedding(vocab_size,d_model)
        pe=torch.zeros((max_len,d_model))
        for pos in range(0,max_len):
            for i in range(0,d_model,2):
                pe[pos,i]=torch.sin(pos/(10000**(i/d_model)))
                pe[pos,i+1]=torch.cos(pos/(10000**(i/d_model)))
        pe=pe.unsqueeze(0) #pe尺寸为(1,max_len,d_model)
        self.register_buffer('pe',pe) #将位置编码注册为缓冲信息，在训练时不随动态计算图更新
        self.dropout=nn.Dropout(p=p_dropout)
        self.max_len=max_len
    def forward(self,input_seq):
        input_seq=self.embedding(input_seq)+self.pe[:,:input_seq.size(1)]
        input_seq=self.dropout(input_seq)
        return input_seq #input_seq为携带位置信息的嵌入张量，尺寸为(batch_size,len_seq,d_model)
def combinemask(tgt_max_len): #定义由paddingmask和futuremask复合构成的掩码向量，仅在decoder中调用
    paddingmask=input_seq!=0
    futuremask=torch.triu(torch.ones(tgt_max_len,tgt_max_len),diagonal=1,dtype=torch.bool)
    futuremask=futuremask.unsqueeze(0).expand(paddingmask.size(0),-1,-1)
    combinemask=paddingmask.unsqueeze(1)&futuremask
    return combinemask
def attention(query,key,value,mask=None,dropout=None): #定义注意力计算方法
    attn_scores=torch.matmul(query,key.transpose(-2,-1))/math.sqrt(query.size(-1)) #未经掩码机制及概率分布化处理的注意力分数，如果query=key=value则输出自注意力分数，如果query与key、value不同则是encoder-decoder交叉注意力分数
    if mask is not None: #应用mask机制。在encoder里调用方法为attention(mask=input_seq!=0)，在decoder里调用方法为attention(mask=combinemask)
        attn_scores=attn_scores.mask_fill(mask==0,-1e9)
        attn_distributions=F.softmax(attn_scores,dim=-1)
        if dropout is not None:
            attn_distributions=dropout(attn_distributions)
        attn_matrix=torch.matmul(attn_distributions,value)
        return attn_matrix,attn_distributions
class MultiHeadAttention(nn.Module): #定义多头注意力模块,计算多头注意力矩阵，输出多头合并后的注意力矩阵，encoder和decoder均可调用
    def __init__(self,num_head,d_model,p_dropout):
        super(MultiHeadAttention,self).__init__()
        self.d_h=d_model//num_head
        self.num_head=num_head
        self.linears=clones(nn.Linear(d_model,d_model),4)
        self.dropout=nn.Dropout(p=p_dropout)
    def forward(self,query,key,value,mask=None):
        query,key,value=[l(x).view(query.size(0),-1,self.num_head,self.d_h).transpose(1,2) for l,x in zip(self.linears,(query,key,value))]
        attn_matrix,attn_distributions=attention(query,key,value,mask=mask,dropout=self.dropout)
        attn_matrix=attn_matrix.transpose(1,2).contiguous().view(query.size(0),-1,d_model)
        attn_matrix=self.linear[-1](attn_matrix)
        return attn_matrix
class LayerNorm(nn.Module): #定义层归一化(LN)模块
    def __init__(self,d_model,eps=1e-10):
        super(LayerNorm,self).__init__()
        self.a=nn.Parameter(torch.ones(d_model)) #缩放矩阵
        self.b=nn.Parameter(torch.zeros(d_model)) #偏移矩阵
        self.eps=eps
    def forward(self,input_seq):
        mean=input_seq.mean(-1,keepdim=True)
        std=input_seq.std(-1,keepdim=True)
        ln_input_seq=self.a*(input_seq-mean)/(std+self.eps)+self.b
        return ln_input_seq
class ResidualConnection(nn.Module): #定义残差连接模块，这里是Pre-LN方法，即先把嵌入向量做层归一化，然后输入给attention层后再做残差连接，效果往往优于《Attention is All you Need》中的Post-LN方法
    def __init__(self,d_model,p_dropout):
        super(ResidualConnection,self).__init__()
        self.norm=LayerNorm(d_model)
        self.dropout=nn.Dropout(p_dropout)
    def forward(self,input_seq,MultiHeadAttention):
        output=input_seq+self.dropout(MultiHeadAttention(self.norm(input_seq)))
        return output
class FeedForward(nn.Module): #定义FFN模块，用于非线性学习
    def __init__(self,d_model,d_ff,p_dropout):
        super(FeedForward,self).__init__()
        self.l_1=nn.Linear(d_model,d_ff)
        self.l_2=nn.Linear(d_ff,d_model)
        self.dropout=nn.Dropout(p_dropout)
    def forward(self,output):
        output=self.w_2(self.dropout(F.relu(self.w_1(output))))
        return output #次为子encoder或子decoder的输出
